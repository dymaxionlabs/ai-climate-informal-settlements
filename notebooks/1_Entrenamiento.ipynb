{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U unetseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-estimator==2.7.0\n",
    "#!pip install opencv-python==4.5.4.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/dymaxionlabs/satproc.git@dab1cdbef0ef3a83c9e100952d9632e17bb18619"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_ Entrenamiento\n",
    "\n",
    "   El modelo de ML utiliza para entrenar un dataset compuesto por imágenes y máscaras para el entrenamiento. Estas ultimas son imagenes binarias que delimitan el objeto de interés, 1 donde esta y 0 donde no. \n",
    "   \n",
    "## Generacion del data set de entrenamiento\n",
    "\n",
    "   En esta etapa generamos el dataset de entrenamiento mediante el uso de **GDAL** y **Satproc**. Primero descargamos las imagenes satelitales y utilizamos GDAL para quedarnos con la combinacion de bandas deseada. Luego Utilizamos satproc para generar el dataset con todas las caracteristicas necesarias para el modelo: tamaño , cantidad de imagenes, rango de valores, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargamos las imágenes del bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos la carpeta a donde descargaremos las imagenes\n",
    "PATH_s1_images_train = \"../images/\"\n",
    "\n",
    "!mkdir -p $PATH_s1_images_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdutil ls --> lista los archivos en un bucket\n",
    "#!gsutil ls gs://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gdutil cp -r --> copia los archivos del bucket al directorio de destino\n",
    "bucket = \"gs://\" \n",
    "\n",
    "!gsutil -m cp -r $bucket   $PATH_s1_images_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar que los archivos fueron correctamente descargados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $PATH_s1_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combinamos todo las imagenes resultantes en un solo vrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdalbuildvrt \\\n",
    "         ../images/file.vrt \\\n",
    "         ../images/*.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satproc\n",
    "\n",
    "Con la herramienta **satproc_extract_chips** se generan, a partir de las imágenes descargadas del bucket, imágenes (chips) generalmente mas pequeñas y máscaras (utilizando las anotaciones de verdad de campo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size =120\n",
    "step_size = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBS** : tanto las imagenes como el archivo vectorial deben tener la misma georeferencia, por ejemplo 4326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files   = PATH_s1_images_train + \"/.vrt\" #carpeta a las imagenes\n",
    "output_folder   = '../dataset/data_train/s1/2021/'+str(size)+'_'+str(step_size)+'/' #carpeta de destino del dataset\n",
    "vector_file     = \"../data/shp/gt/anotcs.geojson\" # archivo vectorial de verdad de campo\n",
    "vector_file_aoi = \"../data/shp/aoi.geojson\" #archivo vectorial con las zonas de interes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Ejecutamos satproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!satproc_extract_chips \\\n",
    "                $path_to_files \\\n",
    "                -o  $output_folder \\\n",
    "                --size $size \\\n",
    "                --step-size $step_size \\\n",
    "                --aoi $vector_file_aoi \\\n",
    "                --labels $vector_file \\\n",
    "                --label-property 'class' \\\n",
    "                --classes 'A' \\\n",
    "                --rescale \\\n",
    "                --rescale-mode values --min -15 --max 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Los argumentos:\n",
    "\n",
    "* **path_to_files** es la ruta a las imágenes \n",
    "\n",
    "* **o** es la ruta de destino \n",
    "\n",
    "Recomendamos que dicha ruta sea descriptiva, por ejemplo “data_train/120_40/ ” describe : Data_train → datos usados para entrenar; 120_40 → <tamaño de la imagen >_ <tamaño del step-size> \n",
    "\n",
    "* **size** tamaño de las imágenes resultantes o chips (las imágenes son cuadradas) \n",
    "* **step-size** paso del proceso. Debe ser menor o igual a *size*. Si *step-size* es igual que el *size* entonces no hay overlap en las imágenes resultantes. \n",
    "\n",
    "En ocaciones es útil para el entrenamiento generar los chips con un overlap de este modo tenemos más datos para entrenar. \n",
    "\n",
    "\n",
    "* **label-property** nombre del campo donde se define cada categoría (solo se usa para el entrenamiento) \n",
    "\n",
    "* **classes** nombres de las clases (como aparecen en el geojson), separados por espacios\n",
    "\n",
    "* **aoi** ruta al archivo vectorial donde están definidas las anotaciones. Al definir una region de interés solo se procesan las imágenes que interceptan esas anotaciones.\n",
    "\n",
    "* **rescale** lleva los valores de las bandas a 0-255 \n",
    "\n",
    "Este comando va a generar dos carpetas en la ruta de destino : “images” y “extent”. Los archivos de la primera van a ser de tipo Tiff de 3 bandas (rgb) y los de la segunda van a ser, también, de tipo Tiff pero de N bandas donde N representa el número de clases, en este caso sólo una. Y donde cada una de las bandas es una máscara binaria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Generamos el entrenamiento del modelo utilizando los datasets creados en el paso previo. El modelo es una red neuronal CNN basado en la arquitectura U-Net. Este considera las imágenes y las máscaras binarias como inputs y genera una imagen con la probabilidad de encontrar al objeto de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unetseg.train import TrainConfig, train\n",
    "from unetseg.evaluate import plot_data_generator\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa debemos definir la configuración del modelo de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(width=160,  #  tamaño de la imagen procesada por la UNet (debe ser multiplos de 16 , por ej 160, 320,etc; y no menor a 80)\n",
    "                     height=160,\n",
    "                     n_channels=3,  #  número de canales de la imagen, rgb -> 3\n",
    "                     n_classes=1, # número de clases a clasificar\n",
    "                     apply_image_augmentation=True, #  si es True , amplia el dataset generando imagenes nuevas a partir de pequeñas variaciones de las ya existentes (rotación,)\n",
    "                     seed=42,\n",
    "                     epochs=20, # Cantidad de veces que el dataset entero puede pasar por el proceso de entrenamiento\n",
    "                     batch_size=16, #cantidad de datos que se procesan por vez, puede ser limitado por la memoria de gpu disponible (debe ser multiplo de 16)\n",
    "                     steps_per_epoch=600, #  típicamente debe ser igual al numero de imágenes / el batch_size, si es mayor incrementara el número de imágenes generadas con image augmentation\n",
    "                     early_stopping_patience=5, # a medida que entrena se guardan los resultados del entrenamiento despues de cada epoch, si el error no varió luego de ¿¿10 ?? iteraciones , se corta el proceso porque se entiende que el error ya disminuyó significativamente \n",
    "                     validation_split=0.2, # se divide la muestra en training (80% de los datos) y validation (20% de los datos) para calcular el error durante el proceso de entrenamiento\n",
    "                     test_split=0.1, # Cantidad de imágenes del dataset\n",
    "                     images_path=os.path.join('../dataset/data_train/s1','2021',str(size)+\"_\"+str(step_size)),#ruta a las imágenes generadas con Satproc\n",
    "                     model_path=os.path.join('../data/weights/', 'UNet_160x160_spe100_3N_spe600_.h5'),#  ruta del modelo entrenado\n",
    "                     model_architecture='unet',\n",
    "                     evaluate=True  ,\n",
    "                     class_weights= [1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Obs*: Es util usar un nombre para el archivo de pesos que de información sobre los parametros de entrenamiento. por ejemplo: < modelo >_< proyecto >_< dim_de_las_imagenes >_< size >_< step_size >_< step_per_epoch >.h5 o similares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar alguna de las imágenes y máscaras del dataset de entrenamiento. A la izquierda se muestra la imágen y a la derecha la máscara "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_generator(num_samples=3, fig_size=(10, 10), train_config=config,img_ch = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_config = train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos las métricas generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(res_config.history['loss'])\n",
    "plt.plot(res_config.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(res_config.history['mean_iou'])\n",
    "plt.plot(res_config.history['val_mean_iou'])\n",
    "plt.title('mean_iou')\n",
    "plt.ylabel('val_mean_iou')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKlws8+6PcuvJrSWdSmTxJ",
   "include_colab_link": true,
   "name": "2 - Entrenamiento",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
